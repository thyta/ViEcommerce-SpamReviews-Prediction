{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7a1819c-ba0a-4653-9038-5ce6b69f24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyvi.ViTokenizer import ViTokenizer #vietnamese tokenizer\n",
    "\n",
    "import re #regular expression\n",
    "\n",
    "import pickle\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cd590a-6646-46a6-93b3-18457fabb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad3f4e5a-34e6-4e50-95fc-feb5deef8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATASET = os.path.join('dataset')\n",
    "\n",
    "EMBEDDING_PATH = os.path.join(DIR_DATASET, 'cc.vi.300.vec')\n",
    "TOKENIZER_PATH = os.path.join(DIR_DATASET, 'tokenizer.pickle')\n",
    "VISTOPWORDS_PATH = os.path.join(DIR_DATASET, \"vietnamese-stopwords.txt\")\n",
    "\n",
    "MODEL_DIR = os.path.join('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d805f32c-df8c-466d-9fe5-5d5d06689825",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNS_PATH = os.path.join(MODEL_DIR, 'CNNs.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e1e3b06-b512-4a66-ada9-1affb9aa61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "textCNNModel = keras.models.load_model(CNNS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed607cc-86bb-4a1d-9978-565d0b1c7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "train_X = np.load('dataset/internal-vocabulary.npy')\n",
    "\n",
    "def convert_np_array(single_string):\n",
    "    tokenizer = text.Tokenizer(lower=False, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(train_X)\n",
    "    input_sequence = tokenizer.texts_to_sequences([single_string])\n",
    "    padded_input_sequence = pad_sequences(input_sequence, maxlen=maxlen)\n",
    "    return padded_input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b71cc7-372e-490f-a5fd-bc0ee05518ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "score:  [[0.9727896]]\n",
      "type: spam\n"
     ]
    }
   ],
   "source": [
    "input_string0 = \"Bá»‰m dÃ¹ng thÃ­ch, mua sale Ä‘Æ°á»£c giÃ¡ tá»‘t shop giao hÃ ng nhanh sáº½ á»§ng há»™ tiáº¿p\"\n",
    "input_string1 = \"Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu\"\n",
    "input_string2 = \"Ã‚jskskxjxndndjsjdjskssosojsjsisossksksjdhhsjsussusjsjsjsjs\"\n",
    "input_string3 = \"CÃ³ 4 MÃ u : Báº¡c - VÃ ng - Äen - VÃ ng Há»“ng âœš Sáº£n Pháº©m : WR âœš TÃ¬nh tráº¡ng : má»›i âœš Äá»“ng há»“ : nam ná»¯ âœš KÃ­ch thÆ°á»›c: 36mm DÃ y : 5mm âœš Cháº¥t liá»‡u khung Ä‘á»“ng há»“: thÃ©p khÃ´ng gá»‰ âœš Cháº¥t liá»‡u kÃ­nh: Chá»‘ng xÆ°á»›c tá»‘t. âœš Loáº¡i dÃ¢y Ä‘eo: thÃ©p khÃ´ng gá»‰ âœš Chá»©c nÄƒng hiá»ƒn thá»‹: Giá», PhÃºt, GiÃ¢y, ngÃ y, thÃ¡ng âœš NÄƒng lÆ°á»£ng: dÃ¹ng pin âœš Kháº£ nÄƒng chá»‘ng nÆ°á»›c: chá»‘ng nÆ°á»›c sinh hoáº¡t âœš Báº£o hÃ nh: 6 thÃ¡ng âž¥ Cam Káº¿t â˜…â˜… ðŸš— GIAO HÃ€NG & THANH TOÃN (COD) Táº¬N NÆ I TRÃŠN TOÃ€N QUá»C. ðŸ‘† ðŸ‘† Báº¢O HÃ€NH MÃY 6 THÃNG, PIN 12 THÃNG, 1 Äá»”I 1 TRONG VÃ’NG 7 NGÃ€Y Náº¾U Sáº¢N PHáº¨M LÃ€ Lá»–I Cá»¦A NHÃ€ Sáº¢N XUáº¤T ðŸ’² KHÃ”NG Báº¢O HÃ€NH TRáº¦Y XÆ¯á»šC BÃŠN NGOÃ€I #dongho #Ä‘á»“nghá»“ná»¯ #sale #sangtrong #donghosapphire #donghothoitrang #donghonu #thoitrang #donghocaocap #donghogiare #donghodoi #Ä‘á»“nghá»“ #Ä‘á»“nghá»“nam #donghonam #chongnuoc\"\n",
    "input_string4 = \"HÃ¬nh áº£nh chá»‰ mang tÃ­nh cháº¥t nháº­n xu \"\n",
    "input_string5 = \"ÄÃºng nhÆ° mÃ´ táº£. MÃ¬nh thÃ­ch mÃ u cam nÃ y. TÃºi Ä‘á»±ng 7inch thoáº£i mÃ¡i. CÃ³ 2 ngÄƒn 2 bÃªn ngoÃ i vÃ  2 ngÄƒn kÃ©o khoÃ¡ á»Ÿ trong.\"\n",
    "input_string6 = \"MÃ u ngoÃ i ko tÆ°Æ¡i nhÆ° hÃ¬nh nhÆ°ng ok. Shop giao nhanh. TÃºi ráº» mÃ  táº­n 4 ngÄƒn, may cÅ©ng cáº©n tháº­n. Size hÆ¡i to so vá»›i Ä‘iá»‡n thoáº¡i, Ä‘Æ°á»£c cÃ¡i nhiá»u ngÄƒn nÃªn Ä‘á»ƒ tiá»n riÃªng sáº¡ch sáº½.\"\n",
    "input_string7 = \"Sáº½ cÃ²n á»§ng há»™ shop, cháº¥t liá»‡u vÃ  Ä‘Æ°á»ng may ráº¥t cháº¯c cháº¯n. HÃ ng tá»« nÆ°á»›c ngoÃ i nhÆ°ng giao khÃ¡ nhanh\"\n",
    "input_string8 = \"TÃºi Ä‘áº¹p láº¯m luÃ´n Ã¡. GiÃ¡ ráº» mÃ  cháº¥t lÆ°á»£ng tá»‘t. LÃºc Ä‘áº§u mÃ¬nh nghÄ© lÃ  nÃ³ kiá»ƒu má»ng má»ng nhÆ°ng mÃ  mua vá» thÃ¬ tháº¥y khÃ´ng nha, dÃ y dáº±n, rá»™ng, Ä‘eo vÃ´ siu Ä‘áº¹p lun Ã¡.\"\n",
    "input_string9 = \"TÃºi rá»™ng rÃ£i, nhiá»u ngÄƒn, váº£i cháº¯c cháº¯n, Ä‘á»±ng Ä‘c Ä‘iá»‡n thoáº¡i vÃ  nhiá»u Ä‘á»“ linh tinh. NhÃ¬n chung okie nhe.\"\n",
    "input_string10 = \"Má»‘i quan há»‡ kiá»ƒu bromosexual chá»§ yáº¿u nháº±m thÃ¡o gá»¡ nhá»¯ng Ä‘á»‹nh kiáº¿n vá» giá»›i tÃ­nh trong cÃ¡ch tÆ°Æ¡ng tÃ¡c vÃ  káº¿t giao báº¡n bÃ¨.\"\n",
    "input_string11 = \"HÃ¬nh áº£nh mang tÃ­nh cháº¥t nháº­n xu, sáº£n pháº©m ok. , sáº½ mua lá»‹a náº¿u cÃ³ nhu cáº§u\"\n",
    "input_string12 = \"Sáº£n pháº©m nhÃ¬n ok, hÃ¬nh áº£nh mang tÃ­nh cháº¥t nháº­n xu\"\n",
    "input_string13 = \"TÃºi pháº£i gá»i lÃ  siÃªu xinh luÃ´n máº¥y bÃ  . Váº£i má»n má»‹n á»Ÿ ngoÃ i xinh hÆ¡n trong hÃ¬nh nha\"\n",
    "input_string14 = \"spammmmmmmmmmmmmmmmmmmmm spammmmmmmmmmmmmm :)))))\"\n",
    "\n",
    "string_convereted = convert_np_array(input_string14)\n",
    "\n",
    "print(string_convereted)\n",
    "\n",
    "predictions = textCNNModel.predict(string_convereted)\n",
    "\n",
    "print(\"score: \", predictions)\n",
    "if predictions > 0.5:\n",
    "    print(\"type: spam\")\n",
    "else:\n",
    "    print(\"type: non-spam\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
