{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a1819c-ba0a-4653-9038-5ce6b69f24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyvi.ViTokenizer import ViTokenizer #vietnamese tokenizer\n",
    "\n",
    "import re #regular expression\n",
    "\n",
    "import pickle\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25cd590a-6646-46a6-93b3-18457fabb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3f4e5a-34e6-4e50-95fc-feb5deef8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATASET = os.path.join('dataset')\n",
    "\n",
    "EMBEDDING_PATH = os.path.join(DIR_DATASET, 'cc.vi.300.vec')\n",
    "TOKENIZER_PATH = os.path.join(DIR_DATASET, 'tokenizer.pickle')\n",
    "VISTOPWORDS_PATH = os.path.join(DIR_DATASET, \"vietnamese-stopwords.txt\")\n",
    "\n",
    "MODEL_DIR = os.path.join('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d805f32c-df8c-466d-9fe5-5d5d06689825",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNS_PATH = os.path.join(MODEL_DIR, 'CNNs.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e1e3b06-b512-4a66-ada9-1affb9aa61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "textCNNModel = keras.models.load_model(CNNS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ed607cc-86bb-4a1d-9978-565d0b1c7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "def convert_np_array(single_string):\n",
    "    tokenizer = text.Tokenizer(lower=False, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts([single_string])\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_string4])\n",
    "    padded_input_sequence = pad_sequences(input_sequence, maxlen=maxlen)\n",
    "    return padded_input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d3b71cc7-372e-490f-a5fd-bc0ee05518ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "input_string0 = \"Bá»‰m dÃ¹ng thÃ­ch, mua sale Ä‘Æ°á»£c giÃ¡ tá»‘t shop giao hÃ ng nhanh sáº½ á»§ng há»™ tiáº¿p\"\n",
    "input_string1 = \"Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu :)))) Xu\"\n",
    "input_string2 = \"Ã‚jskskxjxndndjsjdjskssosojsjsisossksksjdhhsjsussusjsjsjsjs\"\n",
    "input_string3 = \"CÃ³ 4 MÃ u : Báº¡c - VÃ ng - Äen - VÃ ng Há»“ng âœš Sáº£n Pháº©m : WR âœš TÃ¬nh tráº¡ng : má»›i âœš Äá»“ng há»“ : nam ná»¯ âœš KÃ­ch thÆ°á»›c: 36mm DÃ y : 5mm âœš Cháº¥t liá»‡u khung Ä‘á»“ng há»“: thÃ©p khÃ´ng gá»‰ âœš Cháº¥t liá»‡u kÃ­nh: Chá»‘ng xÆ°á»›c tá»‘t. âœš Loáº¡i dÃ¢y Ä‘eo: thÃ©p khÃ´ng gá»‰ âœš Chá»©c nÄƒng hiá»ƒn thá»‹: Giá», PhÃºt, GiÃ¢y, ngÃ y, thÃ¡ng âœš NÄƒng lÆ°á»£ng: dÃ¹ng pin âœš Kháº£ nÄƒng chá»‘ng nÆ°á»›c: chá»‘ng nÆ°á»›c sinh hoáº¡t âœš Báº£o hÃ nh: 6 thÃ¡ng âž¥ Cam Káº¿t â˜…â˜… ðŸš— GIAO HÃ€NG & THANH TOÃN (COD) Táº¬N NÆ I TRÃŠN TOÃ€N QUá»C. ðŸ‘† ðŸ‘† Báº¢O HÃ€NH MÃY 6 THÃNG, PIN 12 THÃNG, 1 Äá»”I 1 TRONG VÃ’NG 7 NGÃ€Y Náº¾U Sáº¢N PHáº¨M LÃ€ Lá»–I Cá»¦A NHÃ€ Sáº¢N XUáº¤T ðŸ’² KHÃ”NG Báº¢O HÃ€NH TRáº¦Y XÆ¯á»šC BÃŠN NGOÃ€I #dongho #Ä‘á»“nghá»“ná»¯ #sale #sangtrong #donghosapphire #donghothoitrang #donghonu #thoitrang #donghocaocap #donghogiare #donghodoi #Ä‘á»“nghá»“ #Ä‘á»“nghá»“nam #donghonam #chongnuoc\"\n",
    "\n",
    "input_string2 = \"em yÃªu anh\"\n",
    "\n",
    "string_convereted = convert_np_array(input_string0)\n",
    "\n",
    "print(string_convereted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71008f62-7053-4b2e-ad1b-bea025dd9faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "score:  [[0.9499721]]\n",
      "type: spam\n"
     ]
    }
   ],
   "source": [
    "predictions = textCNNModel.predict(string_convereted)\n",
    "\n",
    "print(\"score: \", predictions)\n",
    "if predictions > 0.5:\n",
    "    print(\"type: spam\")\n",
    "else:\n",
    "    print(\"type: non-spam\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
